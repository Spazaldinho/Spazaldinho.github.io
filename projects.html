<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Zain's Projects</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="animations.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="dtms.html">Design Teams</a>
            <a href="projects.html">Projects</a>
            <a href="experience.html">Experience</a>
        </nav>
    </header>
    
    <main>
        <section class="projects fade-in">
            <h1>Projects</h1>

            <article class="project">
                <h2>Comparative Analysis of Transformers and Convolutional Recurrent Neural Networks for Image-to-Text Caption Generation: Qmind: Division of AI Research</h2>
                <ul>
                    <li>Investigated image-to-text caption generation, assessing models’ image comprehension.</li>
                    <li>Explored influential solutions like ”Show and Tell” and Andrej Karpathy’s ”Connecting Images and Natural Language” dissertation.</li>
                    <li>Compared convolutional recurrent neural networks and transformers for captioning, evaluating training speed, output speed, and BLEU scores.</li>
                    <li>Utilized the Microsoft COCO dataset with over 200k images, each having five captions.</li>
                    <li>Leveraged Hugging Face tutorials for transformer-based image captioning.</li>
                    <li>Developed an image captioning application incorporating Karpathy’s model and fine-tuned Hugging Face transformers.</li>
                    <li>Explored dense image captioning and model customization when feasible.</li>
                    <li>Employed research methodologies, including data cleaning and architecture design.</li>
                    <li>Utilized cloud computing services (AWS, Microsoft Azure), Pytorch, and HuggingFace Hub.</li>
                    <li>Enhanced web accessibility, allowing screen readers to describe online images for visually impaired individuals.</li>
                </ul>
            </article>

            <article class="project">
                <h2>Autodrive Challenge II: Queen’s Autodrive</h2>
                <ul>
                    <li>Contributed to the AutoDrive Challenge™ II, maintaining the strong partnership between GM and SAE in STEM education.</li>
                    <li>Collaborated with General Motors (GM) representatives and Queen’s University students during the second series of the competition, building upon the achievements of the original four-year event.</li>
                    <li>Participated in the development and demonstration of an autonomous vehicle (AV) capable of urban navigation according to SAE Standard (J3016™) Level 4 automation.</li>
                    <li>Researched and implemented novel computational methods and algorithms to enhance the AV’s perception of its environment.</li>
                    <li>Specialized in designing and implementing Traffic Light Detection and Classification algorithms to ensure accurate recognition and response to traffic signals.</li>
                    <li>Worked collaboratively within a multidisciplinary team, including GM representatives and Queen’s University students, to integrate and optimize perception algorithms within the AV’s software stack.</li>
                    <li>Demonstrated strong problem-solving skills and a dedication to advancing autonomous vehicle technology.</li>
                    <li>Played a pivotal role in enhancing the AV’s safety, reliability, and overall performance in complex urban driving scenarios.</li>
                </ul>
            </article>

            <article class="project">
                <h2>Toyota Innovation Challenge: University of Waterloo Engineering</h2>
                <ul>
                    <li>Developed an image classification project using Python, OpenCV, and deep learning techniques.</li>
                    <li>Collaborated with team members to develop a conceptual image classification solution to identify incorrectly applied or missing components in the manufacturing process of Toyota vehicles.</li>
                    <li>Contributed to the design and implementation of a proof-of-concept model using Python, OpenCV, and deep learning techniques.</li>
                    <li>Focused on exploring image processing, label encoding, and dense neural network architectures using Tensorflow for the proposed solution.</li>
                    <li>Engaged in extensive research and development efforts to create a feasible approach for addressing the manufacturing challenge presented by Toyota.</li>
                    <li>Presented the conceptual solution and findings to judges during the Toyota Innovation Challenge, showcasing the team’s innovative approach to quality control in vehicle manufacturing.</li>
                </ul>
            </article>

            <article class="project">
                <h2>GitHub Developer Tools Automation Analysis: Queen’s University School of Computing</h2>
                <ul>
                    <li>Researched and collected market data to identify common parameters for dependency bots on GitHub.</li>
                    <li>Developed strategies to determine dependency settings for new developers using Python and the GitHub API.</li>
                    <li>Collaborated with a team of researchers to design and implement automation solutions aimed at optimizing software development processes.</li>
                    <li>Conducted in-depth analysis of GitHub repositories to understand the dependencies and trends in open-source projects, contributing to the development of efficient dependency strategies.</li>
                    <li>Presented research findings and automation solutions to a diverse audience, demonstrating the potential for improving software development workflows.</li>
                </ul>
            </article>
        </section>
    </main>

    <footer class="fade-in">
        <!-- Your footer content goes here -->
    </footer>
</body>
</html>
